{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you should try to implement some of the techniques discussed in the lecture.\n",
    "Here is a list of reasonable tasks.\n",
    "\n",
    "Must implement:\n",
    " * Log-loss\n",
    " \n",
    "Easy:\n",
    " * L1 and L2 regularization (you can choose one)\n",
    " * momentum, Nesterov's momentum (you can choose one)\n",
    "\n",
    "Medium difficulty:\n",
    " * Adagrad, RMSProp (you can shoose one) - not much harder than momentum, really\n",
    " * dropout\n",
    "\n",
    "Hard (and time-consuming):\n",
    " * batch-normalization\n",
    "\n",
    "Try to test your network to see if these changes improve accuracy. They improve accuracy much more if you increase the layer size, and if you add more layers, say 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.579  in 0.785964012146\n",
      "Epoch 1: 0.6501  in 0.875180006027\n",
      "Epoch 2: 0.6895  in 0.773838043213\n",
      "Epoch 3: 0.7035  in 0.757263183594\n",
      "Epoch 4: 0.7608  in 0.792301893234\n",
      "Epoch 5: 0.7914  in 0.790660142899\n",
      "Epoch 6: 0.8641  in 0.792309045792\n",
      "Epoch 7: 0.8881  in 0.805778980255\n",
      "Epoch 8: 0.896  in 0.800841093063\n",
      "Epoch 9: 0.903  in 1.17869400978\n"
     ]
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        # initialize biases and weights with random normal distr.\n",
    "        # weights are indexed by target node first\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    def feedforward(self, a):\n",
    "        # Run the network on a batch\n",
    "        a = a.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.matmul(w, a)+b)\n",
    "        return a\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # Update networks weights and biases by applying a single step\n",
    "        # of gradient descent using backpropagation to compute the gradient.\n",
    "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
    "        # eta is the learning rate      \n",
    "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T) # CHANGE: Just one call!\n",
    "            \n",
    "        self.weights = [w-(eta/len(mini_batch[0]))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch[0]))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a pair of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "        g = x\n",
    "        gs = [g] # list to store all the gs, layer by layer\n",
    "        fs = [] # list to store all the fs, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            f = np.dot(w, g)+b\n",
    "            fs.append(f)\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "        # backward pass <- both steps at once\n",
    "        dLdg = self.cost_derivative(gs[-1], y)\n",
    "        dLdfs = []\n",
    "        for w,g in reversed(zip(self.weights,gs[1:])):\n",
    "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
    "            dLdfs.append(dLdf)\n",
    "            dLdg = np.matmul(w.T, dLdf)\n",
    "        \n",
    "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] # automatic here\n",
    "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] # CHANGE: Need to sum here\n",
    "    \n",
    "        return (dLdBs,dLdWs)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
    "        corr = np.argmax(test_data[1],axis=1).T\n",
    "        return sum(pred==corr)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y) \n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        train_size = training_data.images.shape[0]\n",
    "        if test_data:\n",
    "            test_size = test_data.images.shape[0]\n",
    "        for j in xrange(epochs):\n",
    "            t1 = timeit.default_timer()\n",
    "            for k in range(train_size/mini_batch_size):\n",
    "                self.update_mini_batch(training_data.next_batch(mini_batch_size), eta)\n",
    "            if test_data:\n",
    "                res = np.mean([self.evaluate(test_data.next_batch(mini_batch_size)) for k in range(test_size/mini_batch_size)])/mini_batch_size\n",
    "                t2 = timeit.default_timer()\n",
    "                print \"Epoch {0}: {1}  in {2}\".format(j, res,t2-t1)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "\n",
    "network = Network([784,30,10])\n",
    "network.SGD(mnist.train,epochs=10,mini_batch_size=200,eta=3.0,test_data=mnist.test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
